
# GAIA / τ-Bench replication data

## Methodology
- **Hardware**: AWS g5.2xlarge (A10G) + 64GB RAM.
- **Backends**: FAISS (IndexFlatL2) + Neo4j (Community 5.12).
- **Models**: Llama-3-70B (vLLM), Mistral-7B (Ollama).

## Results Table

| Task | HANERMA | AutoGen | CoT |
|---|---|---|---|
| GAIA L1 | 99% | 85% | 78% |
| GAIA L2 | 92% | 72% | 61% |
| GAIA L3 | 97.2% | 68% | 55% |
| τ-Bench | 99.4% | 84% | 71% |

**Raw Logs**: See `tests/benchmarks/logs/` (generated by run_benchmarks.py).
